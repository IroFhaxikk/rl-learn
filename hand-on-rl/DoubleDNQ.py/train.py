from config import Config
from model import Model
from collections import deque
import cv2
import random
import numpy as np
import torch
import torch.nn.functional as F
import torch.optim as optim
import tqdm
import pickle
import os
class Trainer():

    def __init__(self, config:Config, model_class):  # è¿™é‡Œæ¥æ”¶çš„æ˜¯ç±»ï¼Œä¸æ˜¯å®ä¾‹

        self.config = config
        self.env = self.config.env
        self.device = self.config.device
        self.current_epsilon = 1
        self.replay_buffer = deque(maxlen=self.config.repalyBufferSize)
        self.frame_stack = deque(maxlen=4)
        in_channels = 4
        n_actions = self.env.action_space.n

        # ä¿®æ­£ï¼šåˆ›å»ºæ¨¡å‹å®ä¾‹
        self.target_net = model_class(in_channels, n_actions).to(self.config.device)
        self.q_net = model_class(in_channels, n_actions).to(self.config.device)
        
        self.target_net.load_state_dict(self.q_net.state_dict())
        self.target_net.eval()

        self.optimizer = optim.Adam(self.q_net.parameters(), lr=self.config.learning_rate)

        self.initial_frame_stack()

     #åˆå§‹åŒ–4å¸§é˜Ÿåˆ—
    def initial_frame_stack(self):
        state, _        = self.env.reset()
        processed_frame = self.process_frame(state)
        self.frame_stack.clear()
        for _ in range(4):
            self.frame_stack.append(processed_frame)

    #é¢„å¤„ç†ä¸€å¸§
    #input--->frame
    def process_frame(self,state):

        gray_frame       = cv2.cvtColor(state,cv2.COLOR_RGB2GRAY)
        resize_frame     = cv2.resize(gray_frame,(84,84))
        normalized_frame = resize_frame / 255.0
        return normalized_frame.astype(np.float32)
    
    def get_current_framestack(self):
        return np.array(self.frame_stack)   #shape is (4,84,84)
    
    #å…¥é˜Ÿä¸€ä¸ªçŠ¶æ€
    def replaybuffer_push(self,state,action,reward,next_state,done):

        experience = (state,action,reward,next_state,done)
        self.replay_buffer.append(experience)

    #ä»ç»éªŒå›æ”¾ä¸­æŠ½batch_sizeå‡ºæ¥
    def replayBuffer_Sample(self):
        if len(self.replay_buffer) < self.config.batch_size:
            return None

        
        batch = random.sample(self.replay_buffer,self.config.batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)

        states      = torch.FloatTensor(np.array(states)).to(self.device)
        actions     = torch.LongTensor(actions).unsqueeze(1).to(self.device)
        rewards     = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)
        next_states = torch.FloatTensor(np.array(next_states)).to(self.device)
        dones       = torch.FloatTensor(dones).unsqueeze(1).to(self.device)

        return states,actions,rewards,next_states,dones
    
    #é€‰æ‹©åŠ¨ä½œæ ¹æ®Îµ-greedyç­–ç•¥é€‰æ‹©
    def select_action(self,state):
        random_number = random.random()
        if random_number < self.current_epsilon:
            action = self.env.action_space.sample()
        else:
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
                q_value      = self.q_net(state_tensor)
                action       = q_value.argmax(1).item()
        return action
    

    #è¡°å‡epsilon,æ ¹æ®è½®æ•°
    def decay_epsilon(self,episode):
        self.current_epsilon = self.config.min_epsilon + ( self.config.max_epsilon - self.config.min_epsilon ) * np.exp(-self.config.decay_rate * episode)
    
    def save_replay_buffer(self, filepath=r"C:\Users\Administrator\Desktop\rl-learn\savafiles\replay_buffer.pkl"):
        """ä¿å­˜ç»éªŒå›æ”¾ç¼“å†²åŒºåˆ°æ–‡ä»¶"""
        try:
            with open(filepath, 'wb') as f:
                # å°† deque è½¬æ¢ä¸º list å†ä¿å­˜
                buffer_list = list(self.replay_buffer)
                pickle.dump(buffer_list, f)
            print(f"ğŸ’¾ ç»éªŒå›æ”¾ç¼“å†²åŒºå·²ä¿å­˜åˆ° {filepath}ï¼ŒåŒ…å« {len(buffer_list)} æ¡ç»éªŒ")
            return True
        except Exception as e:
            print(f" ä¿å­˜ç»éªŒå›æ”¾ç¼“å†²åŒºå¤±è´¥: {e}")
            return False
    
    def load_replay_buffer(self, filepath=r"C:\Users\Administrator\Desktop\rl-learn\savafiles\replay_buffer.pkl"):
        """ä»æ–‡ä»¶åŠ è½½ç»éªŒå›æ”¾ç¼“å†²åŒº"""
        try:
            if os.path.exists(filepath):
                with open(filepath, 'rb') as f:
                    buffer_list = pickle.load(f)
                self.replay_buffer.clear()
                self.replay_buffer.extend(buffer_list)
                print(f"ğŸ“¥ ç»éªŒå›æ”¾ç¼“å†²åŒºå·²ä» {filepath} åŠ è½½ï¼ŒåŒ…å« {len(self.replay_buffer)} æ¡ç»éªŒ")
                return True
            else:
                print(f" ç»éªŒå›æ”¾ç¼“å†²åŒºæ–‡ä»¶ {filepath} ä¸å­˜åœ¨")
                return False
        except Exception as e:
            print(f" åŠ è½½ç»éªŒå›æ”¾ç¼“å†²åŒºå¤±è´¥: {e}")
            return False
    
    def prefill_relaybuffer(self, force_refill=False):
        """é¢„å¡«å……ç»éªŒå›æ”¾ç¼“å†²åŒºï¼Œå¦‚æœå·²æœ‰æ•°æ®åˆ™è·³è¿‡"""
        # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰è¶³å¤Ÿçš„æ•°æ®
        if not force_refill and len(self.replay_buffer) >= self.config.repalyBufferSize * 0.8:
            print(f" ç»éªŒå›æ”¾ç¼“å†²åŒºå·²æœ‰ {len(self.replay_buffer)} æ¡æ•°æ®ï¼Œè·³è¿‡é¢„å¡«å……")
            return
        
        print(" æ­£åœ¨é¢„å¡«å……ç»éªŒå›æ”¾ç¼“å†²åŒº...")
        self.initial_frame_stack()
        state = self.get_current_framestack()
        
        fill_target = self.config.repalyBufferSize
        current_size = len(self.replay_buffer)
        
        # åªå¡«å……åˆ°ç›®æ ‡å¤§å°
        for i in range(fill_target - current_size):
            action = self.env.action_space.sample()
            next_state_frame, reward, done, truncated, info = self.env.step(action)
            processed_frame = self.process_frame(next_state_frame)
            self.frame_stack.append(processed_frame)
            next_state = self.get_current_framestack()
            
            self.replaybuffer_push(state, action, reward, next_state, done)
            
            if done:
                self.initial_frame_stack()
                state = self.get_current_framestack()
            else:
                state = next_state
            
            # æ˜¾ç¤ºè¿›åº¦
            if (i + 1) % 1000 == 0:
                print(f"  å¡«å……è¿›åº¦: {current_size + i + 1}/{fill_target}")
        
        print(f" ç»éªŒå›æ”¾é¢„å¡«å……å®Œæˆï¼Œå½“å‰æ€»é‡: {len(self.replay_buffer)} æ¡")
                
    def train(self):
        # æ¢åˆ° config çš„åˆå§‹ epsilon
        self.current_epsilon = getattr(self.config, "max_epsilon", 1.0)

        # ä¼˜åŒ–å™¨ä¸æŸå¤±ï¼ˆä½ åœ¨ __init__ å·²åˆ›å»º optimizer / loss_fnï¼Œå¯ç”¨ self.optimizer, self.loss_fnï¼‰
        # å¦‚æœæƒ³ç”¨ Huber lossï¼šloss_fn = F.smooth_l1_loss
        loss_fn = F.smooth_l1_loss  # æ›´ç¨³å¥ä¹Ÿå¸¸ç”¨

        global_step = 0  # ç”¨å…¨å±€æ­¥æ•°æ¥æ§åˆ¶ target æ›´æ–°ã€ä¿å­˜ç­‰

        for episode in tqdm.tqdm(range(self.config.n_training_episodes)):
            # reset ç¯å¢ƒå¹¶åˆå§‹åŒ– frame stackï¼ˆä½¿ state ä¸º 4 å¸§å †å ï¼‰
            obs, info = self.env.reset()
            self.initial_frame_stack()
            state = self.get_current_framestack()  # numpy (4,84,84)

            episode_reward = 0.0
            done = False
            truncated = False
            step = 0    
            total_loss = 0.0

            # æŠŠ q_net è®¾ä¸ºè®­ç»ƒæ¨¡å¼ï¼ˆdropout/bn ç­‰ï¼‰
            self.q_net.train()
            while not done and not truncated:
                # ---------------- 1) é€‰æ‹©åŠ¨ä½œï¼ˆÎµ-greedyï¼‰
                action = self.select_action(state)   # ä¼ å…¥ stateï¼ˆ4å¸§å †å ï¼‰ï¼Œä¸æ˜¯å•å¸§ obs

                # ---------------- 2) ä¸ç¯å¢ƒäº¤äº’ï¼Œæ›´æ–° frame stack
                next_obs, reward, done, truncated, info = self.env.step(action)
                processed_obs = self.process_frame(next_obs)
                self.frame_stack.append(processed_obs)
                next_state = self.get_current_framestack()

                # ---------------- 3) å­˜ç»éªŒ
                self.replaybuffer_push(state, action, reward, next_state, done)

                # ---------------- 4) è®­ç»ƒï¼šä» replay buffer é‡‡æ ·å¹¶æ›´æ–°ç½‘ç»œ
                batch = self.replayBuffer_Sample()
                if batch is not None:
                    states, actions, rewards, next_states, dones = batch
                    # states: (B,4,84,84) tensors on device

                    # Double DQN ç›®æ ‡è®¡ç®—
                    # current Q for taken actions
                    q_values = self.q_net(states).gather(1, actions)  # (B,1)

                    # online net selects the best next action
                    next_actions = self.q_net(next_states).argmax(1).unsqueeze(1)  # (B,1)

                    # target net evaluates that action
                    next_q_values = self.target_net(next_states).gather(1, next_actions)  # (B,1)

                    # compute TD target (detach next_q_values)
                    target_q = rewards + (1.0 - dones) * self.config.gamma * next_q_values.detach()  # (B,1)

                    # loss (Huber)
                    loss = loss_fn(q_values, target_q)

                    # backward + step
                    self.optimizer.zero_grad()
                    loss.backward()
                    # å¯é€‰ï¼šæ¢¯åº¦è£å‰ª
                    if getattr(self.config, "max_grad_norm", None) is not None:
                        torch.nn.utils.clip_grad_norm_(self.q_net.parameters(), self.config.max_grad_norm)
                    self.optimizer.step()
                    total_loss += loss.item()
                # ---------------- 5) æ›´æ–°è®¡æ•°ã€target ç½‘ç»œã€epsilonã€state
                step += 1
                global_step += 1
                self.decay_epsilon(episode)
                episode_reward += reward
                state = next_state

                # ç”¨å…¨å±€æ­¥æ•°æ›´æ–° target_netï¼ˆæ›´å‡åŒ€ï¼‰ï¼Œä½¿ç”¨ config.target_update_freqï¼ˆé»˜è®¤1000ï¼‰
                if getattr(self.config, "target_update_freq", 1000) > 0:
                    if global_step % self.config.target_update_freq == 0:
                        self.target_net.load_state_dict(self.q_net.state_dict())

                if done or truncated:
                    break

            if episode % 10 == 0 and episode > 0:
                avg_loss = total_loss / (step + 1e-8)
                print(f"\nEpisode {episode:6d} | "
                        f"Reward: {episode_reward:.2f} | "
                        f"Epsilon: {self.current_epsilon:.4f} | "
                        f"Loss: {avg_loss:.6f} | "
                        f"ReplayBuffer: {len(self.replay_buffer)}")
                
